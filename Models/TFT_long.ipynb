{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFT (Long forecasting)\n",
    "#### Sliding Window Forecasting - 3 years in, 1 year out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need for runnning colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio pandas numpy scikit-learn pytorch-forecasting\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# # Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correct the file path to the actual location of the CSV file\n",
    "df = pd.read_csv('/Users/casper/Documents/GitHub/p9-energy/Dataset/ConsumptionIndustry.csv', sep=';')\n",
    "\n",
    "# Load the dataset for colab\n",
    "# df = pd.read_csv('ConsumptionIndustry.csv', sep=';')\n",
    "\n",
    "\n",
    "# Convert HourDK to datetime\n",
    "df['HourDK'] = pd.to_datetime(df['HourDK'])\n",
    "\n",
    "# Convert ConsumptionkWh to numeric\n",
    "df['ConsumptionkWh'] = df['ConsumptionkWh'].str.replace(\",\", \".\").astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preperation + Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            HourUTC              HourDK  MunicipalityNo Branche  \\\n",
      "0  2021-01-01 00:00 2021-01-01 01:00:00             851  Privat   \n",
      "1  2021-01-01 01:00 2021-01-01 02:00:00             851  Privat   \n",
      "2  2021-01-01 02:00 2021-01-01 03:00:00             851  Privat   \n",
      "3  2021-01-01 03:00 2021-01-01 04:00:00             851  Privat   \n",
      "4  2021-01-01 04:00 2021-01-01 05:00:00             851  Privat   \n",
      "\n",
      "   ConsumptionkWh  ConsumptionkWh_lag1  ConsumptionkWh_lag24  \\\n",
      "0       35086.772                  NaN                   NaN   \n",
      "1       31777.762            35086.772                   NaN   \n",
      "2       28423.659            31777.762                   NaN   \n",
      "3       25675.926            28423.659                   NaN   \n",
      "4       24283.909            25675.926                   NaN   \n",
      "\n",
      "   ConsumptionkWh_lag168  ConsumptionkWh_roll24  ConsumptionkWh_roll168  \\\n",
      "0                    NaN                    NaN                     NaN   \n",
      "1                    NaN                    NaN                     NaN   \n",
      "2                    NaN                    NaN                     NaN   \n",
      "3                    NaN                    NaN                     NaN   \n",
      "4                    NaN                    NaN                     NaN   \n",
      "\n",
      "   is_holiday  day_of_week  is_weekend  hour_sin  hour_cos   day_sin  \\\n",
      "0        True            4           0  0.258819  0.965926 -0.433884   \n",
      "1        True            4           0  0.500000  0.866025 -0.433884   \n",
      "2        True            4           0  0.707107  0.707107 -0.433884   \n",
      "3        True            4           0  0.866025  0.500000 -0.433884   \n",
      "4        True            4           0  0.965926  0.258819 -0.433884   \n",
      "\n",
      "    day_cos  month_sin  month_cos  time_idx  \n",
      "0 -0.900969        0.5   0.866025       0.0  \n",
      "1 -0.900969        0.5   0.866025       1.0  \n",
      "2 -0.900969        0.5   0.866025       2.0  \n",
      "3 -0.900969        0.5   0.866025       3.0  \n",
      "4 -0.900969        0.5   0.866025       4.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33839 entries, 0 to 33838\n",
      "Data columns (total 20 columns):\n",
      " #   Column                  Non-Null Count  Dtype         \n",
      "---  ------                  --------------  -----         \n",
      " 0   HourUTC                 33839 non-null  object        \n",
      " 1   HourDK                  33839 non-null  datetime64[ns]\n",
      " 2   MunicipalityNo          33839 non-null  int64         \n",
      " 3   Branche                 33839 non-null  object        \n",
      " 4   ConsumptionkWh          33839 non-null  float64       \n",
      " 5   ConsumptionkWh_lag1     33838 non-null  float64       \n",
      " 6   ConsumptionkWh_lag24    33815 non-null  float64       \n",
      " 7   ConsumptionkWh_lag168   33671 non-null  float64       \n",
      " 8   ConsumptionkWh_roll24   33816 non-null  float64       \n",
      " 9   ConsumptionkWh_roll168  33672 non-null  float64       \n",
      " 10  is_holiday              33839 non-null  bool          \n",
      " 11  day_of_week             33839 non-null  int32         \n",
      " 12  is_weekend              33839 non-null  int64         \n",
      " 13  hour_sin                33839 non-null  float64       \n",
      " 14  hour_cos                33839 non-null  float64       \n",
      " 15  day_sin                 33839 non-null  float64       \n",
      " 16  day_cos                 33839 non-null  float64       \n",
      " 17  month_sin               33839 non-null  float64       \n",
      " 18  month_cos               33839 non-null  float64       \n",
      " 19  time_idx                33839 non-null  float64       \n",
      "dtypes: bool(1), datetime64[ns](1), float64(13), int32(1), int64(2), object(2)\n",
      "memory usage: 4.8+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9x/yl6kp0_j23s302pc3z00dpjh0000gn/T/ipykernel_13644/2801619376.py:39: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_time_index = pd.date_range(\n",
      "/var/folders/9x/yl6kp0_j23s302pc3z00dpjh0000gn/T/ipykernel_13644/2801619376.py:50: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_full['ConsumptionkWh'].fillna(method='ffill', inplace=True)\n",
      "/var/folders/9x/yl6kp0_j23s302pc3z00dpjh0000gn/T/ipykernel_13644/2801619376.py:50: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_full['ConsumptionkWh'].fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "df['HourDK'] = pd.to_datetime(df['HourDK'])\n",
    "\n",
    "# Create lag features, rolling averages, and other engineered features\n",
    "df['ConsumptionkWh_lag1'] = df['ConsumptionkWh'].shift(1)\n",
    "df['ConsumptionkWh_lag24'] = df['ConsumptionkWh'].shift(24)\n",
    "df['ConsumptionkWh_lag168'] = df['ConsumptionkWh'].shift(168)\n",
    "df['ConsumptionkWh_roll24'] = df['ConsumptionkWh'].rolling(window=24).mean()\n",
    "df['ConsumptionkWh_roll168'] = df['ConsumptionkWh'].rolling(window=168).mean()\n",
    "\n",
    "holidays = [\n",
    "    '2021-01-01', '2021-04-01', '2021-04-02', '2021-04-05', '2021-05-13', '2021-05-21',\n",
    "    # Add more holidays here...\n",
    "]\n",
    "holidays = pd.to_datetime(holidays)\n",
    "df['is_holiday'] = df['HourDK'].dt.date.isin(holidays.date)\n",
    "df['day_of_week'] = df['HourDK'].dt.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['HourDK'].dt.hour / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['HourDK'].dt.hour / 24)\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['HourDK'].dt.month / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['HourDK'].dt.month / 12)\n",
    "\n",
    "# Assuming hourly data, use 'h' for hours\n",
    "df['time_idx'] = (df['HourDK'] - df['HourDK'].min()) // np.timedelta64(1, 'h')\n",
    "\n",
    "\n",
    "# Drop rows with NaN values after feature engineering\n",
    "##df.dropna(inplace=True)\n",
    "\n",
    "# Ensure 'HourDK' is in datetime format\n",
    "df['HourDK'] = pd.to_datetime(df['HourDK'])\n",
    "\n",
    "# Create a full time index (hourly frequency from min to max time in your dataset)\n",
    "full_time_index = pd.date_range(\n",
    "    df['HourDK'].min(), df['HourDK'].max(), freq='H')\n",
    "\n",
    "# Create a DataFrame with this full time index\n",
    "df_full = pd.DataFrame(full_time_index, columns=['HourDK'])\n",
    "\n",
    "# Merge with the existing data to find missing values\n",
    "df_full = df_full.merge(df, on='HourDK', how='left')\n",
    "\n",
    "# Fill or handle missing values\n",
    "# Forward fill missing consumption values\n",
    "df_full['ConsumptionkWh'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Create a new time index as an integer (starting from 0 for the first timestamp)\n",
    "df['time_idx'] = (df['HourDK'] - df['HourDK'].min()\n",
    "                  ).dt.total_seconds() // 3600  # Hours as integers\n",
    "\n",
    "# Now use df_full for model training\n",
    "train_df = df_full\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 21839 rows\n",
      "Validation Set: 3650 rows\n",
      "Test Set: 8065 rows\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset split ranges\n",
    "train_start = \"2021-01-01\"\n",
    "train_end = \"2023-06-30\"\n",
    "val_start = \"2023-07-01\"\n",
    "val_end = \"2023-11-30\"\n",
    "test_start = \"2023-12-01\"\n",
    "test_end = \"2024-11-01\"\n",
    "\n",
    "# Split datasets\n",
    "train_df = df[(df['HourDK'] >= train_start) & (df['HourDK'] <= train_end)]\n",
    "val_df = df[(df['HourDK'] >= val_start) & (df['HourDK'] <= val_end)]\n",
    "test_df = df[(df['HourDK'] >= test_start) & (df['HourDK'] <= test_end)]\n",
    "\n",
    "print(f\"Training Set: {train_df.shape[0]} rows\")\n",
    "print(f\"Validation Set: {val_df.shape[0]} rows\")\n",
    "print(f\"Test Set: {test_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the TFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Timeseries index should be of type integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m max_prediction_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m \u001b[38;5;66;03m# 1 day\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#Traning Dataset \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTimeSeriesDataSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtime_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConsumptionkWh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43mgroup_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHourDK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_encoder_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_prediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_prediction_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtime_varying_known_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhour_sin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhour_cos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mday_sin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mday_cos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmonth_sin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmonth_cos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_holiday\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtime_varying_unknown_reals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConsumptionkWh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConsumptionkWh_lag1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConsumptionkWh_lag24\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConsumptionkWh_roll24\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtarget_normalizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGroupNormalizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHourDK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoftplus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[43madd_relative_time_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m  \u001b[49m\u001b[43madd_target_scales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m  \u001b[49m\u001b[43madd_encoder_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#Validation Dataset\u001b[39;00m\n\u001b[1;32m     23\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TimeSeriesDataSet\u001b[38;5;241m.\u001b[39mfrom_dataset(train_dataset, val_df)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/testing_env/lib/python3.9/site-packages/pytorch_forecasting/data/timeseries.py:351\u001b[0m, in \u001b[0;36mTimeSeriesDataSet.__init__\u001b[0;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_prediction_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin prediction length must be larger than 0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_prediction_length, \u001b[38;5;28mint\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin prediction length must be integer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data[time_idx]\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeseries index should be of type integer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m target\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m weight\n",
      "\u001b[0;31mAssertionError\u001b[0m: Timeseries index should be of type integer"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting import GroupNormalizer, TimeSeriesDataSet \n",
    "\n",
    "max_encoder_length = 336 # 14 days\n",
    "max_prediction_length = 24 # 1 day\n",
    "\n",
    "#Traning Dataset \n",
    "train_dataset = TimeSeriesDataSet(\n",
    "  train_df,\n",
    "  time_idx=\"time_idx\",\n",
    "  target=\"ConsumptionkWh\",\n",
    "  group_ids=[\"HourDK\"],\n",
    "  max_encoder_length=max_encoder_length, \n",
    "  max_prediction_length=max_prediction_length,\n",
    "  time_varying_known_reals=[\"hour_sin\", \"hour_cos\", \"day_sin\", \"day_cos\", \"month_sin\", \"month_cos\", \"is_holiday\"],\n",
    "  time_varying_unknown_reals=[\"ConsumptionkWh\", \"ConsumptionkWh_lag1\", \"ConsumptionkWh_lag24\", \"ConsumptionkWh_roll24\"],\n",
    "  target_normalizer=GroupNormalizer(groups=[\"HourDK\"], transformation=\"softplus\"),\n",
    "  add_relative_time_idx=True,\n",
    "  add_target_scales=True,\n",
    "  add_encoder_length=True,\n",
    ")\n",
    "\n",
    "#Validation Dataset\n",
    "val_dataset = TimeSeriesDataSet.from_dataset(train_dataset, val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train the TFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import QuantileLoss\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "\n",
    "# Define the model \n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "  dataset=train_dataset,\n",
    "  learning_rate=0.01, \n",
    "  hidden_size=64,\n",
    "  attention_head_size=4,\n",
    "  dropout=0.1,\n",
    "  hidden_continuous_size=32,\n",
    "  output_size=7, # 7 quantiles by default\n",
    "  loss=QuantileLoss(),\n",
    "  log_interval=10,\n",
    "  reduce_on_plateau_patience=4\n",
    ")\n",
    "\n",
    "# Train the model \n",
    "trainer = tft.trainer(\n",
    "  max_epochs=10,\n",
    "  gpus=1 if torch.cuda.is_available() else 0,\n",
    "  gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "trainer.fit(tft, train_dataloader=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the test period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataset\n",
    "test_dataset = TimeSeriesDataSet.from_dataset(train_dataset, test_df)\n",
    "\n",
    "# Create test DataLoader\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Predict using the trained model\n",
    "raw_predictions, x = tft.predict(test_dataloader, return_x=True)\n",
    "\n",
    "# Extract predictions\n",
    "predictions = raw_predictions.numpy()\n",
    "\n",
    "# Map predictions back to the original test dataframe\n",
    "predicted_dates = test_df[\"HourDK\"].iloc[-len(predictions):].reset_index(drop=True)\n",
    "predicted_df = pd.DataFrame({\"Date\": predicted_dates, \"Predicted_ConsumptionkWh\": predictions})\n",
    "\n",
    "# True values (actual consumption) from the test set\n",
    "# Adjust based on the length of predictions\n",
    "true_values = test_df[\"ConsumptionkWh\"].values[-len(predictions):]\n",
    "\n",
    "# Predictions (model's predicted consumption)\n",
    "predictions = predicted_df[\"Predicted_ConsumptionkWh\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate error on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the metrics\n",
    "mae = mean_absolute_error(true_values, predictions)\n",
    "mse = mean_squared_error(true_values, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Mean Absolute Scaled Error (MASE)\n",
    "mase = mean_absolute_error(true_values, predictions) / \\\n",
    "    np.mean(np.abs(np.diff(true_values)))\n",
    "\n",
    "# Symmetric Mean Absolute Percentage Error (sMAPE)\n",
    "sMAPE = 100 * np.mean(2 * np.abs(predictions - true_values) /\n",
    "                      (np.abs(true_values) + np.abs(predictions)))\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE)\n",
    "mape = 100 * np.mean(np.abs(predictions - true_values) / np.abs(true_values))\n",
    "\n",
    "# R-squared score\n",
    "r2 = r2_score(true_values, predictions)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Energy Transformer Model Performance:\")\n",
    "print(\"Total Training time: 9min 23sec\")  # Adjust as needed\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")\n",
    "print('\\n')\n",
    "print(f\"Mean Absolute Scaled Error: {mase:.2f}\")\n",
    "print(f\"Symmetric Mean Absolute Percentage Error: {sMAPE:.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error: {mape:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_df[\"HourDK\"], test_df[\"ConsumptionkWh\"],\n",
    "         label=\"Actual\", alpha=0.8)\n",
    "plt.plot(predicted_df[\"Date\"], predicted_df[\"Predicted_ConsumptionkWh\"],\n",
    "         label=\"Predicted\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.title(\"Electricity Consumption Predictions (1/12/2023 - 1/11/2024)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Consumption (kWh)\")\n",
    "plt.show()\n",
    "\n",
    "predicted_df.to_csv(\"predictions_2023_12_to_2024_11.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
