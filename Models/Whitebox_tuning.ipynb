{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\sebas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\sebas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\sebas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.14.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sebas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\sebas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: prophet in c:\\users\\sebas\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.1.6)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas statsmodels matplotlib seaborn prophet sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.statespace.dynamic_factor_mq import DynamicFactorMQ\n",
    "from statsmodels.tsa.forecasting.stl import STLForecast\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.forecasting.theta import ThetaModel\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "try:\n",
    "  from google.colab import files\n",
    "  from google.colab import drive\n",
    "  uploaded = files.upload()\n",
    "  !mkdir -p \"/content/drive/My Drive/p9\"\n",
    "  drive.mount('/content/drive')\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "import warnings\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumption data\n",
    "df = pd.read_csv('../Dataset/ConsumptionIndustry.csv' if not IN_COLAB else 'ConsumptionIndustry.csv', sep=';')\n",
    "\n",
    "df['HourDK'] = pd.to_datetime(df['HourDK'])\n",
    "df['ConsumptionkWh'] = df['ConsumptionkWh'].str.replace(\",\", \".\").astype(float)\n",
    "df.index = df['HourDK']\n",
    "\n",
    "# format data here\n",
    "df.drop(columns=['HourUTC', 'HourDK', 'MunicipalityNo', 'Branche'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El-spot prices\n",
    "df2 = pd.read_csv('../Dataset/ELSpotPrices.csv' if not IN_COLAB else 'ELSpotPrices.csv', sep=';')\n",
    "df2['HourDK'] = pd.to_datetime(df2['HourDK'])\n",
    "df2['SpotPriceDKK'] = df2['SpotPriceDKK'].str.replace(\",\", \".\").astype(float)\n",
    "df2.index = df2['HourDK']\n",
    "df2 = df2.iloc[1:] # remove first row, since the measurement at that time is not present in other dataset\n",
    "df2.drop(columns=['HourUTC', 'HourDK', 'PriceArea', 'SpotPriceEUR'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data_train, data_test, predictions, save_at=''):\n",
    "  plt.figure(figsize=(7, 3))\n",
    "  plt.plot(data_train.index, data_train, label=f'Train ({data_train.index[0]} - {data_train.index[-1]})')\n",
    "  plt.plot(data_test.index, data_test, label=f'Test ({data_test.index[0]} - {data_test.index[-1]})')\n",
    "  plt.plot(data_test.index, predictions, label='Prediction')\n",
    "  plt.title('Consumption in danish private households with prediction')\n",
    "  plt.xlabel('Measurements')\n",
    "  plt.ylabel('Power (kW / charger)')\n",
    "  plt.legend()\n",
    "  if save_at:\n",
    "    plt.savefig(save_at)\n",
    "  plt.show()\n",
    "\n",
    "def sample_data_with_train_window(df, start_date, train_window_size):\n",
    "  start_date = datetime.strptime(start_date, '%Y-%m-%d') - timedelta(hours=train_window_size)\n",
    "  end_date = df.index[-1]\n",
    "  return df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "\n",
    "def get_next_window(data, train_window_size, forecast_horizon):\n",
    "  return data[:train_window_size], data[train_window_size:train_window_size + forecast_horizon]\n",
    "\n",
    "def forecast_whitebox_model(model, forecast_horizon, model_name, exog_data_test=None):\n",
    "  model_res = model.fit()\n",
    "\n",
    "  if \"SARIMA\" in model_name and \"STL\" not in model_name:\n",
    "    return model_res.get_forecast(steps=forecast_horizon, exog=exog_data_test).predicted_mean\n",
    "  else:\n",
    "    return model_res.forecast(steps=forecast_horizon)\n",
    "\n",
    "def create_result_table(results, columns=[]):\n",
    "  result_table = pd.DataFrame(results)\n",
    "  result_table.columns = columns\n",
    "  result_table = result_table.sort_values(by='rmse', ascending=True).reset_index(drop=True)\n",
    "  return result_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_SARIMA(data_train, data_test, forecast_horizon, model_name):\n",
    "  results = []\n",
    "  best_rmse = 0\n",
    "  p = d = q = range(0, 3)\n",
    "  seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "  for param in itertools.product(p, d, q):\n",
    "    for seasonal_param in seasonal_pdq:\n",
    "      try:\n",
    "          model = SARIMAX(data_train, order=param, seasonal_order=seasonal_param)\n",
    "          predictions = forecast_whitebox_model(model, forecast_horizon, model_name)\n",
    "      except:\n",
    "          continue\n",
    "      \n",
    "      rmse = root_mean_squared_error(data_test, predictions)\n",
    "      results.append([f\"{param}x{seasonal_param}\", rmse])\n",
    "      print(f\"{param}x{seasonal_param} - RMSE: {rmse}\")\n",
    "\n",
    "      if rmse < best_rmse or best_rmse == 0:\n",
    "        best_prediction = predictions\n",
    "\n",
    "  result_table = create_result_table(results, columns=['params', 'rmse'])\n",
    "  return result_table, best_prediction\n",
    "\n",
    "def optimize_SARIMAX(data_train, data_test, forecast_horizon, model_name, exog_data_train=None, exog_data_test=None):\n",
    "  results = []\n",
    "  best_rmse = 0\n",
    "  p = d = q = range(0, 3)\n",
    "  seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "  for param in itertools.product(p, d, q):\n",
    "    for seasonal_param in seasonal_pdq:\n",
    "      try:\n",
    "          model = SARIMAX(data_train, order=param, seasonal_order=seasonal_param, exog=exog_data_train)\n",
    "          predictions = forecast_whitebox_model(model, forecast_horizon, model_name, exog_data_test=exog_data_test)\n",
    "      except:\n",
    "          continue\n",
    "      \n",
    "      rmse = root_mean_squared_error(data_test, predictions)\n",
    "      results.append([f\"{param}x{seasonal_param}\", rmse])\n",
    "      print(f\"{param}x{seasonal_param} - RMSE: {rmse}\")\n",
    "\n",
    "      if rmse < best_rmse or best_rmse == 0:\n",
    "        best_prediction = predictions\n",
    "\n",
    "  result_table = create_result_table(results, columns=['params', 'rmse'])\n",
    "  return result_table, best_prediction\n",
    "\n",
    "def optimize_Theta_model(data_train, data_test, forecast_horizon, model_name):\n",
    "  results = []\n",
    "  best_rmse = 0\n",
    "  p = range(1, 25)\n",
    "  d = [True, False]\n",
    "  u = [True, False]\n",
    "  m = ['additive', 'multiplicative']\n",
    "  di = [True, False]\n",
    "\n",
    "  for param in itertools.product(p, d, u, m, di):\n",
    "    try:\n",
    "      model = ThetaModel(data_train, period=param[0], deseasonalize=param[1], use_test=param[2], method=param[3], difference=param[4])\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "    predictions = forecast_whitebox_model(model, forecast_horizon, model_name)\n",
    "    rmse = root_mean_squared_error(data_test, predictions)\n",
    "    results.append([param, rmse])\n",
    "    print(f\"{param} - RMSE: {rmse}\")\n",
    "    \n",
    "    if rmse < best_rmse or best_rmse == 0:\n",
    "      best_prediction = predictions\n",
    "\n",
    "  result_table = create_result_table(results, columns=['params', 'rmse'])\n",
    "  return result_table, best_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing through whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0)x(0, 0, 0, 12) - RMSE: 8.134767913360665e-12\n",
      "(0, 0, 0)x(0, 0, 1, 12) - RMSE: 8.134767913360665e-12\n",
      "(0, 0, 0)x(0, 0, 2, 12) - RMSE: 5.250970108951386e-12\n",
      "(0, 0, 0)x(0, 1, 0, 12) - RMSE: 2.348305222286955e-12\n",
      "(0, 0, 0)x(0, 1, 2, 12) - RMSE: 2.348305222286955e-12\n",
      "(0, 0, 0)x(0, 2, 0, 12) - RMSE: 6.966199054705344e-12\n"
     ]
    }
   ],
   "source": [
    "model_name = 'SARIMAX'\n",
    "date_start = '2023-11-01'\n",
    "window_train_size = 24*7*2 #hours\n",
    "forecast_horizon = 24 #hours\n",
    "data = sample_data_with_train_window(df, date_start, window_train_size) # start: date_start - window_train_size, end: last date in df\n",
    "exog_data = sample_data_with_train_window(df2, date_start, window_train_size)\n",
    "\n",
    "data_train, data_test = get_next_window(data, window_train_size, forecast_horizon)\n",
    "exog_data_train, exog_data_test = get_next_window(exog_data, window_train_size, forecast_horizon)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "result, pred = optimize_SARIMAX(data_train, data_test, forecast_horizon, model_name, exog_data_train=exog_data_train, exog_data_test=exog_data_test)\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "if IN_COLAB:\n",
    "  plot_data(data_train, data_test, pred, save_at=f'/content/drive/My Drive/p9/{window_train_size}_{forecast_horizon}_{model_name}.png')\n",
    "  result.to_csv(f'/content/drive/My Drive/p9/{window_train_size}_{forecast_horizon}_{model_name}.csv')\n",
    "else:\n",
    "  plot_data(data_train, data_test, pred, save_at=f'../Results/Whitebox/Tuning/{window_train_size}_{forecast_horizon}_{model_name}.png')\n",
    "  result.to_csv(f'../Results/Whitebox/Tuning/{window_train_size}_{forecast_horizon}_{model_name}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
