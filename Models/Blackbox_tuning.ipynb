{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import optuna\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import LSTM, Informer, NHITS, DLinear\n",
    "from neuralforecast.losses.pytorch import RMSE\n",
    "from neuralforecast.losses.pytorch import DistributionLoss\n",
    "from pytorch_forecasting import MAE\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('once')\n",
    "\n",
    "import os\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddataset():\n",
    "    # consumption = pd.read_csv('ConsumptionIndustry.csv', sep=';')\n",
    "    # spot_prices = pd.read_csv('/content/ELSpotPrices.csv', sep=';')\n",
    "    consumption = pd.read_csv('../Dataset/ConsumptionIndustry.csv', sep=';')\n",
    "    spot_prices = pd.read_csv('../Dataset/ELSpotPrices.csv', sep=';')\n",
    "\n",
    "    # Convert comma decimal format to float\n",
    "    consumption['ConsumptionkWh'] = consumption['ConsumptionkWh'].str.replace(\n",
    "        ',', '.').astype(float)\n",
    "    spot_prices['SpotPriceDKK'] = spot_prices['SpotPriceDKK'].str.replace(\n",
    "        ',', '.').astype(float)\n",
    "\n",
    "    # Remove first row, since the measurement at that time is not present in other dataset\n",
    "    spot_prices = spot_prices.iloc[1:]\n",
    "\n",
    "    # Merge datasets on HourDK\n",
    "    combined_data = pd.merge(consumption, spot_prices,\n",
    "                             on='HourDK', how='inner')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    combined_data = combined_data.drop(\n",
    "        ['HourUTC_x', 'HourUTC_y', 'SpotPriceEUR', 'MunicipalityNo', 'Branche', 'PriceArea'], axis=1)\n",
    "\n",
    "    # Set HourDK as index\n",
    "    combined_data.index = pd.to_datetime(\n",
    "        combined_data['HourDK'])  # Ensure index is datetime\n",
    "\n",
    "    combined_data['HourDK'] = pd.to_datetime(combined_data['HourDK'])\n",
    "    combined_data['Hour'] = combined_data['HourDK'].dt.hour\n",
    "    combined_data['DayOfWeek'] = combined_data['HourDK'].dt.dayofweek\n",
    "    combined_data['IsWeekend'] = combined_data['DayOfWeek'].isin([\n",
    "                                                                 5, 6]).astype(int)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def prepare_neuralforecast_data(combined_data):\n",
    "    # Reset index while avoiding duplicates\n",
    "    combined_data = combined_data.reset_index(drop=True)\n",
    "\n",
    "    # Rename columns to fit neuralforecast conventions\n",
    "    combined_data = combined_data.rename(\n",
    "        columns={'HourDK': 'ds', 'ConsumptionkWh': 'y'})\n",
    "\n",
    "    # Add unique_id for a single time series\n",
    "    # Single series; use unique values if there are multiple series\n",
    "    combined_data['unique_id'] = 1\n",
    "\n",
    "    combined_data.index = pd.to_datetime(combined_data['ds'])\n",
    "\n",
    "    return combined_data[['unique_id', 'ds', 'y'] + [col for col in combined_data.columns if col not in ['unique_id', 'ds', 'y']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_with_train_window(df, start_date, end_date, train_window_size):\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "        df.index = pd.to_datetime(df['ds'])\n",
    "\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d') - timedelta(hours=train_window_size) + timedelta(hours=24)\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(hours=24)\n",
    "\n",
    "    return df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "\n",
    "def get_next_window(data, train_window_size, forecast_horizon):\n",
    "  return data[:train_window_size], data[train_window_size:train_window_size + forecast_horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_LSTM(trial, data_train, data_test, forecast_horizon):\n",
    "    nf = NeuralForecast(\n",
    "        models=[LSTM(h=forecast_horizon, input_size=-1, loss=RMSE(),\n",
    "                    encoder_n_layers=trial.suggest_categorical('encoder_n_layers', [1, 2, 5, 10]),\n",
    "                    encoder_hidden_size=trial.suggest_categorical('encoder_hidden_size', [100, 200, 300, 400]),\n",
    "                    context_size=trial.suggest_categorical('context_size', [5, 10, 15, 20]),\n",
    "                    decoder_hidden_size=trial.suggest_categorical('decoder_hidden_size', [100, 200, 300, 400]),\n",
    "                    decoder_layers=trial.suggest_categorical('decoder_layers', [1, 2, 5, 10]),\n",
    "                    max_steps=trial.suggest_categorical('max_steps', [200, 500, 1000, 3000]),\n",
    "                    val_check_steps=trial.suggest_categorical('val_check_steps', [10, 20, 50, 100, 250, 500]),\n",
    "                    batch_size=trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "                    scaler_type=trial.suggest_categorical('scaler_type', ['standard', 'minmax', 'robust']),\n",
    "                    )\n",
    "        ],\n",
    "        freq='H'\n",
    "    )\n",
    "    nf.fit(data_train)\n",
    "    predictions = nf.predict(data_test)\n",
    "    return root_mean_squared_error(data_test['y'], predictions['LSTM'])\n",
    "\n",
    "def objective_Informer(trial, data_train, data_test, forecast_horizon):\n",
    "    nf = NeuralForecast(\n",
    "        models=[Informer(\n",
    "                    h=forecast_horizon, input_size=24, loss=RMSE(),\n",
    "                    hidden_size=trial.suggest_categorical('hidden_size', [8, 16, 32, 64, 128, 256]),\n",
    "                    conv_hidden_size=trial.suggest_categorical('conv_hidden_size', [8, 16, 32, 64, 128, 256]),\n",
    "                    n_head=trial.suggest_categorical('n_head', [1, 2, 4, 8]),\n",
    "                    scaler_type=trial.suggest_categorical('scaler_type', ['standard', 'minmax', 'robust']),\n",
    "                    max_steps=5\n",
    "                    )\n",
    "        ],\n",
    "        freq='H'\n",
    "    )\n",
    "    nf.fit(data_train)\n",
    "    predictions = nf.predict(data_test)\n",
    "    return root_mean_squared_error(data_test['y'], predictions['Informer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-02 22:33:01,978] A new study created in memory with name: no-name-8325c8d9-0da0-4ef8-9e5b-2cb88039af67\n",
      "Seed set to 1\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type          | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | loss          | RMSE          | 0      | train\n",
      "1 | padder_train  | ConstantPad1d | 0      | train\n",
      "2 | scaler        | TemporalNorm  | 0      | train\n",
      "3 | enc_embedding | DataEmbedding | 768    | train\n",
      "4 | dec_embedding | DataEmbedding | 768    | train\n",
      "5 | encoder       | TransEncoder  | 792 K  | train\n",
      "6 | decoder       | TransDecoder  | 561 K  | train\n",
      "--------------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.423     Total estimated model params size (MB)\n",
      "73        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0900e61286714f069ceb22c8a08251a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a331faeb71445b999fc0fc5338dd0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "date_start = '2023-11-01'\n",
    "date_end = '2024-11-01'\n",
    "window_train_size = 1440 #hours\n",
    "forecast_horizon = 336 #hours\n",
    "# 336_24, 1440_336, 17520_8760\n",
    "trials = 1\n",
    "model_name = f'LSTM_{window_train_size}_{forecast_horizon}'\n",
    "\n",
    "combined_data = loaddataset()\n",
    "neuralforecast_data = prepare_neuralforecast_data(combined_data)\n",
    "data = sample_data_with_train_window(neuralforecast_data, date_start, date_end, window_train_size)\n",
    "data_train, data_test = get_next_window(data, window_train_size, forecast_horizon)\n",
    "\n",
    "def safe_objective(trial):\n",
    "  try:\n",
    "    return objective_LSTM(trial, data_train, data_test, forecast_horizon)\n",
    "  except Exception as e:\n",
    "    print(f\"Failed trial: {e}. Skipped this trial.\")\n",
    "    return float('inf')\n",
    "  \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "study1 = optuna.create_study(direction='minimize')\n",
    "study1.optimize(safe_objective, n_trials=trials)\n",
    "\n",
    "trial=study1.best_trial\n",
    "print(f\"Accuracy: {trial.value}\")\n",
    "print(f\"best params for {model_name}: {trial.params}\")\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "# Save the results in CSV\n",
    "if trial.value != float('inf'):\n",
    "  try:\n",
    "    df_tuning = pd.read_csv('../Results/blackbox_tuning.csv')\n",
    "  except:\n",
    "    df_tuning = pd.DataFrame(columns=['model', 'accuracy', 'params'])\n",
    "\n",
    "  new_row = {'model': model_name, 'accuracy': trial.value, 'params': str(trial.params)}\n",
    "  new_row_df = pd.DataFrame([new_row]).dropna(axis=1, how='all')\n",
    "  df_tuning = pd.concat([df_tuning, new_row_df], ignore_index=True)\n",
    "  df_tuning = df_tuning.sort_values(by=['model', 'accuracy', 'params'], ascending=True).reset_index(drop=True)\n",
    "  df_tuning.to_csv('../Results/blackbox_tuning.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
