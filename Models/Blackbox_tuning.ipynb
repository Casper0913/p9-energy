{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import LSTM, Informer, NHITS, DLinear\n",
    "from neuralforecast.losses.pytorch import RMSE\n",
    "from neuralforecast.losses.pytorch import DistributionLoss\n",
    "from pytorch_forecasting import MAE\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('once')\n",
    "\n",
    "import os\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddataset():\n",
    "    # consumption = pd.read_csv('ConsumptionIndustry.csv', sep=';')\n",
    "    # spot_prices = pd.read_csv('/content/ELSpotPrices.csv', sep=';')\n",
    "    consumption = pd.read_csv('../Dataset/ConsumptionIndustry.csv', sep=';')\n",
    "    spot_prices = pd.read_csv('../Dataset/ELSpotPrices.csv', sep=';')\n",
    "\n",
    "    # Convert comma decimal format to float\n",
    "    consumption['ConsumptionkWh'] = consumption['ConsumptionkWh'].str.replace(\n",
    "        ',', '.').astype(float)\n",
    "    spot_prices['SpotPriceDKK'] = spot_prices['SpotPriceDKK'].str.replace(\n",
    "        ',', '.').astype(float)\n",
    "\n",
    "    # Remove first row, since the measurement at that time is not present in other dataset\n",
    "    spot_prices = spot_prices.iloc[1:]\n",
    "\n",
    "    # Merge datasets on HourDK\n",
    "    combined_data = pd.merge(consumption, spot_prices,\n",
    "                             on='HourDK', how='inner')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    combined_data = combined_data.drop(\n",
    "        ['HourUTC_x', 'HourUTC_y', 'SpotPriceEUR', 'MunicipalityNo', 'Branche', 'PriceArea'], axis=1)\n",
    "\n",
    "    # Set HourDK as index\n",
    "    combined_data.index = pd.to_datetime(\n",
    "        combined_data['HourDK'])  # Ensure index is datetime\n",
    "\n",
    "    combined_data['HourDK'] = pd.to_datetime(combined_data['HourDK'])\n",
    "    combined_data['Hour'] = combined_data['HourDK'].dt.hour\n",
    "    combined_data['DayOfWeek'] = combined_data['HourDK'].dt.dayofweek\n",
    "    combined_data['IsWeekend'] = combined_data['DayOfWeek'].isin([\n",
    "                                                                 5, 6]).astype(int)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def prepare_neuralforecast_data(combined_data):\n",
    "    # Reset index while avoiding duplicates\n",
    "    combined_data = combined_data.reset_index(drop=True)\n",
    "\n",
    "    # Rename columns to fit neuralforecast conventions\n",
    "    combined_data = combined_data.rename(\n",
    "        columns={'HourDK': 'ds', 'ConsumptionkWh': 'y'})\n",
    "\n",
    "    # Add unique_id for a single time series\n",
    "    # Single series; use unique values if there are multiple series\n",
    "    combined_data['unique_id'] = 1\n",
    "\n",
    "    combined_data.index = pd.to_datetime(combined_data['ds'])\n",
    "\n",
    "    return combined_data[['unique_id', 'ds', 'y'] + [col for col in combined_data.columns if col not in ['unique_id', 'ds', 'y']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_with_train_window(df, start_date, end_date, train_window_size):\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "        df.index = pd.to_datetime(df['ds'])\n",
    "\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d') - timedelta(hours=train_window_size) + timedelta(hours=24)\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(hours=24)\n",
    "\n",
    "    return df[(df.index >= start_date) & (df.index <= end_date)]\n",
    "\n",
    "def get_next_window(data, train_window_size, forecast_horizon):\n",
    "  return data[:train_window_size], data[train_window_size:train_window_size + forecast_horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_LSTM(trial, data_train, data_test, forecast_horizon):\n",
    "    nf = NeuralForecast(\n",
    "        models=[LSTM(h=forecast_horizon, input_size=-1, loss=RMSE(),\n",
    "                    encoder_n_layers=trial.suggest_categorical('encoder_n_layers', [1, 2, 5, 10]),\n",
    "                    encoder_hidden_size=trial.suggest_categorical('encoder_hidden_size', [100, 200, 300, 400]),\n",
    "                    context_size=trial.suggest_categorical('context_size', [5, 10, 15, 20]),\n",
    "                    decoder_hidden_size=trial.suggest_categorical('decoder_hidden_size', [100, 200, 300, 400]),\n",
    "                    decoder_layers=trial.suggest_categorical('decoder_layers', [1, 2, 5, 10]),\n",
    "                    max_steps=trial.suggest_categorical('max_steps', [200, 500, 1000, 3000]),\n",
    "                    val_check_steps=trial.suggest_categorical('val_check_steps', [10, 20, 50, 100, 250, 500]),\n",
    "                    batch_size=trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "                    scaler_type=trial.suggest_categorical('scaler_type', ['standard', 'minmax', 'robust']),\n",
    "                    )\n",
    "        ],\n",
    "        freq='H'\n",
    "    )\n",
    "    nf.fit(data_train)\n",
    "    predictions = nf.predict(data_test)\n",
    "    return root_mean_squared_error(data_test['y'], predictions['LSTM'])\n",
    "\n",
    "def objective_Informer(trial, data_train, data_test, forecast_horizon):\n",
    "    nf = NeuralForecast(\n",
    "        models=[Informer(h=forecast_horizon, loss=RMSE(),\n",
    "                    input_size=trial.suggest_categorical('input_size', [1, 2, 6, 12, 24, 48]),\n",
    "                    hidden_size=trial.suggest_categorical('hidden_size', [128/2, 128, 128*2, 128*3]),\n",
    "                    n_head=trial.suggest_categorical('n_head', [1, 2, 4, 8]),\n",
    "                    conv_hidden_size=trial.suggest_categorical('conv_hidden_size', [16, 32, 64, 128, 256]),\n",
    "                    encoder_layers=trial.suggest_categorical('encoder_layers', [1, 2, 3, 4]),\n",
    "                    decoder_layers=trial.suggest_categorical('decoder_layers', [1, 2, 3, 4]),\n",
    "                    max_steps=trial.suggest_categorical('max_steps', [200, 500, 1000, 3000]),\n",
    "                    val_check_steps=trial.suggest_categorical('val_check_steps', [10, 20, 50, 100, 250, 500]),\n",
    "                    batch_size=trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "                    )\n",
    "        ],\n",
    "        freq='H'\n",
    "    )\n",
    "    nf.fit(data_train)\n",
    "    predictions = nf.predict(data_test)\n",
    "    return root_mean_squared_error(data_test['y'], predictions['Informer'])\n",
    "\n",
    "def objective_NHITS(trial, data_train, data_test, forecast_horizon):\n",
    "    nf = NeuralForecast(\n",
    "        models=[NHITS(h=forecast_horizon, loss=RMSE(),\n",
    "                    input_size=trial.suggest_categorical('input_size', [1, 2, 6, 12, 24, 48]),\n",
    "                    max_steps=trial.suggest_categorical('max_steps', [200, 500, 1000, 3000]),\n",
    "                    val_check_steps=trial.suggest_categorical('val_check_steps', [10, 20, 50, 100, 250, 500]),\n",
    "                    batch_size=trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "                    step_size=trial.suggest_categorical('step_size', [1, 2, 3, 4, 5]),\n",
    "                    scaler_type=trial.suggest_categorical('scaler_type', ['standard', 'minmax', 'robust', 'identity']),\n",
    "                    )\n",
    "        ],\n",
    "        freq='H'\n",
    "    )\n",
    "    nf.fit(data_train)\n",
    "    predictions = nf.predict(data_test)\n",
    "    return root_mean_squared_error(data_test['y'], predictions['NHITS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-04 13:13:12,010] A new study created in memory with name: no-name-41029806-008d-40a0-beb6-6da8228682b0\n",
      "Seed set to 1\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type          | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | loss          | RMSE          | 0      | train\n",
      "1 | padder_train  | ConstantPad1d | 0      | train\n",
      "2 | scaler        | TemporalNorm  | 0      | train\n",
      "3 | enc_embedding | DataEmbedding | 384    | train\n",
      "4 | dec_embedding | DataEmbedding | 384    | train\n",
      "5 | encoder       | TransEncoder  | 216 K  | train\n",
      "6 | decoder       | TransDecoder  | 598 K  | train\n",
      "--------------------------------------------------------\n",
      "814 K     Trainable params\n",
      "0         Non-trainable params\n",
      "814 K     Total params\n",
      "3.260     Total estimated model params size (MB)\n",
      "136       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193c802ccb004723bdbd30f538862160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b7ec99035349628c3b17db645b655c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
      "[I 2025-01-04 13:14:19,862] Trial 0 finished with value: inf and parameters: {'input_size': 12, 'hidden_size': 128, 'n_head': 2, 'conv_hidden_size': 64, 'encoder_layers': 2, 'decoder_layers': 4, 'max_steps': 200, 'val_check_steps': 100, 'batch_size': 32}. Best is trial 0 with value: inf.\n",
      "Seed set to 1\n",
      "[I 2025-01-04 13:14:19,901] Trial 1 finished with value: inf and parameters: {'input_size': 1, 'hidden_size': 256, 'n_head': 4, 'conv_hidden_size': 32, 'encoder_layers': 3, 'decoder_layers': 2, 'max_steps': 3000, 'val_check_steps': 50, 'batch_size': 16}. Best is trial 0 with value: inf.\n",
      "Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed trial: name 'exit' is not defined. Skipped this trial.\n",
      "Failed trial: Check decoder_input_size_multiplier=0.5, range (0,1). Skipped this trial.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type          | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | loss          | RMSE          | 0      | train\n",
      "1 | padder_train  | ConstantPad1d | 0      | train\n",
      "2 | scaler        | TemporalNorm  | 0      | train\n",
      "3 | enc_embedding | DataEmbedding | 384    | train\n",
      "4 | dec_embedding | DataEmbedding | 384    | train\n",
      "5 | encoder       | TransEncoder  | 678 K  | train\n",
      "6 | decoder       | TransDecoder  | 199 K  | train\n",
      "--------------------------------------------------------\n",
      "878 K     Trainable params\n",
      "0         Non-trainable params\n",
      "878 K     Total params\n",
      "3.515     Total estimated model params size (MB)\n",
      "109       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8b4786c6ab4364a525d61e59ff1dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8133531efd4edd98af4966d39ecfee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "date_start = '2023-11-01'\n",
    "date_end = '2024-11-01'\n",
    "window_train_size = 336 #hours\n",
    "forecast_horizon = 24 #hours\n",
    "# 336_24, 1440_336, 17520_8760\n",
    "trials = 20\n",
    "model_name = f'Informer_{window_train_size}_{forecast_horizon}'\n",
    "\n",
    "combined_data = loaddataset()\n",
    "neuralforecast_data = prepare_neuralforecast_data(combined_data)\n",
    "data = sample_data_with_train_window(neuralforecast_data, date_start, date_end, window_train_size)\n",
    "data_train, data_test = get_next_window(data, window_train_size, forecast_horizon)\n",
    "\n",
    "def safe_objective(trial):\n",
    "  try:\n",
    "    return objective_Informer(trial, data_train, data_test, forecast_horizon)\n",
    "  except Exception as e:\n",
    "    print(f\"Failed trial: {e}. Skipped this trial.\")\n",
    "    return float('inf')\n",
    "  \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "study1 = optuna.create_study(direction='minimize')\n",
    "study1.optimize(safe_objective, n_trials=trials)\n",
    "\n",
    "trial=study1.best_trial\n",
    "print(f\"Accuracy: {trial.value}\")\n",
    "print(f\"best params for {model_name}: {trial.params}\")\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "# Save the results in CSV\n",
    "if trial.value != float('inf'):\n",
    "  try:\n",
    "    df_tuning = pd.read_csv('../Results/blackbox_tuning.csv')\n",
    "  except:\n",
    "    df_tuning = pd.DataFrame(columns=['model', 'accuracy', 'params'])\n",
    "\n",
    "  new_row = {'model': model_name, 'accuracy': trial.value, 'params': str(trial.params)}\n",
    "  new_row_df = pd.DataFrame([new_row]).dropna(axis=1, how='all')\n",
    "  df_tuning = pd.concat([df_tuning, new_row_df], ignore_index=True)\n",
    "  df_tuning = df_tuning.sort_values(by=['model', 'accuracy', 'params'], ascending=True).reset_index(drop=True)\n",
    "  df_tuning.to_csv('../Results/blackbox_tuning.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
